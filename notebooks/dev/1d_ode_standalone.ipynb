{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fc804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd53a33",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InterpolationDataset: for sanity-checking that the networks can interpolate the GT solution function\n",
    "class InterpolationDataset(Dataset):\n",
    "    def __init__(self, num_points, domain=(-1, 1), function=torch.sin, sampling='equispaced', noise_std=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_points: Number of points to sample\n",
    "            domain: Tuple of (min, max) for the domain\n",
    "            function: Function to apply to x values\n",
    "            sampling: String indicating sampling method ('equispaced', 'chebyshev', or 'random')\n",
    "        \"\"\"\n",
    "        self.num_points = num_points\n",
    "        self.domain = domain\n",
    "        self.function = function\n",
    "        self.sampling = sampling\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        # Sample\n",
    "        self.resample()\n",
    "    \n",
    "    def _generate_points(self):\n",
    "        valid_methods = {'equispaced', 'chebyshev', 'random'}\n",
    "        if self.sampling not in valid_methods:\n",
    "            raise ValueError(f\"Sampling method must be one of {valid_methods}\")\n",
    "        if self.sampling == 'equispaced':\n",
    "            x = torch.linspace(self.domain[0], self.domain[1], self.num_points)\n",
    "        elif self.sampling == 'chebyshev':\n",
    "            j = torch.arange(self.num_points)\n",
    "            x = torch.cos((2*j + 1) * torch.pi / (2*self.num_points))\n",
    "            x = self._scale_to_domain(x, self.domain)\n",
    "        else:  # random\n",
    "            x = torch.rand(self.num_points) * (self.domain[1] - self.domain[0]) + self.domain[0]\n",
    "        return x\n",
    "    \n",
    "    def _scale_to_domain(self, x, domain):\n",
    "        \"\"\"Scale points from [-1, 1] to specified domain\"\"\"\n",
    "        return (domain[1] - domain[0]) * (x + 1) / 2 + domain[0]\n",
    "    \n",
    "    def resample(self):\n",
    "        \"\"\"Regenerate x points and corresponding y values\"\"\"\n",
    "        self.x = self._generate_points()\n",
    "        self.y = self.function(self.x)\n",
    "        self.y += torch.randn_like(self.y) * self.noise_std\n",
    "        return self\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdf6d4",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=32, activation=torch.tanh):\n",
    "        \"\"\"\n",
    "        2-layer MLP that maps R -> R\n",
    "        \n",
    "        Args:\n",
    "            hidden_dim: Dimension of hidden layer\n",
    "            activation: Activation function to use (default: ReLU)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ensure input is 2D tensor [batch_size, 1]\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(-1)\n",
    "            \n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze(-1)  # Return [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a41bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagrangeInterpolationModel(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        \"\"\"\n",
    "        Model parameterized by values at Chebyshev points of the second kind\n",
    "        \n",
    "        Args:\n",
    "            num_points: Number of points (N where N is number of points)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Generate Chebyshev points of the second kind\n",
    "        i = torch.linspace(0, 1, num_points)\n",
    "        self.nodes = torch.cos(torch.pi * i)  # [-1, 1]\n",
    "        \n",
    "        # Learnable values at these points\n",
    "        self.values = nn.Parameter(torch.zeros(num_points))\n",
    "        \n",
    "        # Precompute barycentric weights for Chebyshev points\n",
    "        self.weights = torch.zeros(num_points)\n",
    "        self.weights[::2] = 1\n",
    "        self.weights[1::2] = -1\n",
    "        self.weights[0] = 0.5\n",
    "        self.weights[-1] = 0.5\n",
    "        \n",
    "        # Cache for differentiation matrices\n",
    "        self._diff_matrices = {}\n",
    "    \n",
    "    def _cheb_interpolate(self, x_eval, values, eps=1e-14):\n",
    "        \"\"\"\n",
    "        Interpolate values from nodes to x_eval using barycentric formula\n",
    "        \n",
    "        Args:\n",
    "            x_eval: points to evaluate at (N_eval)\n",
    "            values: values at self.nodes to interpolate from\n",
    "            eps: tolerance for detecting exact node matches\n",
    "        Returns:\n",
    "            interpolated values at x_eval\n",
    "        \"\"\"\n",
    "        # Compute difference matrix (N, N_eval)\n",
    "        d_x = x_eval.unsqueeze(0) - self.nodes.unsqueeze(1)\n",
    "        \n",
    "        # Handle numerical instability for small differences\n",
    "        small_diff = torch.abs(d_x) < eps\n",
    "        small_diff_max = torch.max(small_diff, dim=0).values\n",
    "        \n",
    "        # If small_diff, set the column to 0 and the entry to 1\n",
    "        d_x = torch.where(\n",
    "            small_diff_max[None, :],\n",
    "            torch.zeros_like(d_x),\n",
    "            1.0 / d_x\n",
    "        )\n",
    "        d_x[small_diff] = 1\n",
    "        \n",
    "        # Interpolate\n",
    "        f_eval_num = torch.einsum(\"...n,nm,n->...m\", values, d_x, self.weights)\n",
    "        f_eval_denom = torch.einsum(\"nm,n->m\", d_x, self.weights)\n",
    "        \n",
    "        return f_eval_num / f_eval_denom\n",
    "\n",
    "    def _compute_derivative_matrix(self, nodes):\n",
    "        \"\"\"\n",
    "        Compute the spectral differentiation matrix for Chebyshev points of the second kind\n",
    "        using the stable barycentric formula.\n",
    "        \"\"\"\n",
    "        n = len(nodes)\n",
    "        D = torch.zeros((n, n), dtype=nodes.dtype, device=nodes.device)\n",
    "        \n",
    "        # Compute weights for endpoints\n",
    "        c = torch.ones(n, dtype=nodes.dtype, device=nodes.device)\n",
    "        c[0] = 2\n",
    "        c[-1] = 2\n",
    "        \n",
    "        # Compute off-diagonal entries\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    D[i,j] = c[i]/c[j] * (-1)**(i+j) / (nodes[i] - nodes[j])\n",
    "        \n",
    "        # Fill diagonal using negative sum trick\n",
    "        D.diagonal().copy_(-torch.sum(D, dim=1))\n",
    "        \n",
    "        return D\n",
    "        \n",
    "    def derivative_matrix(self, k=1):\n",
    "        \"\"\"\n",
    "        Get k-th derivative matrix (cached for efficiency)\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            n = len(self.nodes)\n",
    "            return torch.eye(n, dtype=self.nodes.dtype, device=self.nodes.device)\n",
    "            \n",
    "        if k not in self._diff_matrices:\n",
    "            # Compute first derivative matrix if not cached\n",
    "            if 1 not in self._diff_matrices:\n",
    "                self._diff_matrices[1] = self._compute_derivative_matrix(self.nodes)\n",
    "            \n",
    "            # Compute k-th derivative matrix by composition\n",
    "            Dk = self._diff_matrices[1]\n",
    "            for i in range(k-1):\n",
    "                Dk = Dk @ self._diff_matrices[1]\n",
    "            self._diff_matrices[k] = Dk\n",
    "            \n",
    "        return self._diff_matrices[k]\n",
    "        \n",
    "    def derivative(self, x_eval, k=1):\n",
    "        \"\"\"\n",
    "        Compute k-th derivative of interpolant at x_eval points\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            return self(x_eval)\n",
    "            \n",
    "        # Get k-th derivative matrix\n",
    "        Dk = self.derivative_matrix(k)\n",
    "        \n",
    "        # Compute derivative values at nodes (differentiable w.r.t self.values)\n",
    "        dk_nodes = Dk @ self.values\n",
    "        \n",
    "        # Interpolate to evaluation points\n",
    "        return self._cheb_interpolate(x_eval, dk_nodes)\n",
    "    \n",
    "    def forward(self, x_eval):\n",
    "        return self._cheb_interpolate(x_eval, self.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cb759",
   "metadata": {},
   "source": [
    "## Sanity check: create model and set to interpolate sin(x). Are the derivatives accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe990f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: create model and set to interpolate sin(x). Are the derivatives accurate?\n",
    "n_points = 21\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "model.values.data = torch.sin(model.nodes)\n",
    "\n",
    "# Test points\n",
    "x_eval = torch.linspace(-1, 1, 200)\n",
    "\n",
    "# Compute derivatives\n",
    "derivs = [model.derivative(x_eval, k) for k in range(4)]\n",
    "\n",
    "# True derivatives\n",
    "true_funcs = [\n",
    "    torch.sin,\n",
    "    torch.cos,\n",
    "    lambda x: -torch.sin(x),\n",
    "    lambda x: -torch.cos(x)\n",
    "]\n",
    "true_derivs = [f(x_eval) for f in true_funcs]\n",
    "\n",
    "# Plot errors\n",
    "plt.figure(figsize=(15, 5))\n",
    "for k in range(4):\n",
    "    error = torch.abs(derivs[k] - true_derivs[k])\n",
    "    plt.semilogy(x_eval, error.detach().cpu().numpy(), label=f'{k}-th derivative')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Derivative Errors')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.show()\n",
    "\n",
    "# Print max errors\n",
    "print(\"\\nMaximum errors:\")\n",
    "for k in range(4):\n",
    "    error = torch.max(torch.abs(derivs[k] - true_derivs[k]))\n",
    "    print(f\"{k}-th derivative: {error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb6d0f3",
   "metadata": {},
   "source": [
    "## PINN training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ead9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivative(model, x, eval_mode=False):\n",
    "    if isinstance(model, LagrangeInterpolationModel):\n",
    "        u = model(x)\n",
    "        du = model.derivative(x, k=1)\n",
    "    else:\n",
    "        # For MLP, compute gradient manually\n",
    "        x_clone = x.clone().requires_grad_(True)\n",
    "        u = model(x_clone)\n",
    "        # During eval, we don't need create_graph\n",
    "        du = torch.autograd.grad(u.sum(), x_clone, \n",
    "                               create_graph=not eval_mode)[0]\n",
    "        if eval_mode:\n",
    "            u = u.detach()\n",
    "            du = du.detach()\n",
    "    return u, du\n",
    "\n",
    "# PINN training utils\n",
    "def compute_pde_loss(model, colloc_points, boundary_weight=1.0, u0=0):\n",
    "    \"\"\"\n",
    "    Compute loss for ODE u' = u + x with u(0) = 0\n",
    "    \"\"\"\n",
    "    u, du = compute_derivative(model, colloc_points)\n",
    "        \n",
    "    # PDE residual: u' = u + x\n",
    "    pde_residual = du - u - colloc_points\n",
    "        \n",
    "    pde_loss = torch.mean(pde_residual**2)\n",
    "    \n",
    "    # Boundary condition: u(0) = u0\n",
    "    bc_point = torch.tensor([0.0], dtype=torch.float64)\n",
    "    bc_residual = model(bc_point)-u0\n",
    "    bc_loss = boundary_weight * bc_residual**2\n",
    "    \n",
    "    return pde_loss + bc_loss, pde_residual, bc_residual\n",
    "\n",
    "# # Torch implementation\n",
    "# def running_min(x):\n",
    "#     return torch.cummin(x, dim=0)[0]\n",
    "\n",
    "# Simple iterative approach\n",
    "def running_min(lst):\n",
    "    result = []\n",
    "    current_min = float('inf')\n",
    "    for x in lst:\n",
    "        current_min = min(current_min, x)\n",
    "        result.append(current_min)\n",
    "    return result\n",
    "\n",
    "def train_pinn(model, n_colloc=100, n_epochs=1000, lr=1e-3, boundary_weight=1.0, \n",
    "                colloc_sampling='equispaced', u0=0):\n",
    "    \"\"\"\n",
    "    Train model to solve the ODE\n",
    "    \n",
    "    Args:\n",
    "        model: neural network model\n",
    "        n_colloc: number of collocation points\n",
    "        n_epochs: number of training epochs\n",
    "        lr: learning rate\n",
    "        boundary_weight: weight for boundary condition term\n",
    "        colloc_sampling: sampling strategy for collocation points\n",
    "            - 'equispaced': evenly spaced points\n",
    "            - 'chebyshev': Chebyshev points of second kind\n",
    "            - 'random': uniformly random points, not resampled\n",
    "            - 'random_resample': uniformly random points, resampled each epoch\n",
    "    \"\"\"\n",
    "    def get_colloc_points():\n",
    "        if colloc_sampling == 'equispaced':\n",
    "            points = torch.linspace(-1, 1, n_colloc, dtype=torch.float64)\n",
    "        elif colloc_sampling == 'chebyshev':\n",
    "            i = torch.linspace(0, 1, n_colloc, dtype=torch.float64)\n",
    "            points = torch.cos(torch.pi * i)\n",
    "        elif colloc_sampling == 'random' or colloc_sampling == 'random_resample':\n",
    "            # Random points in [-1, 1]\n",
    "            points = 2 * torch.rand(n_colloc, dtype=torch.float64) - 1\n",
    "            # Sort points for better visualization\n",
    "            points, _ = torch.sort(points)\n",
    "        elif colloc_sampling == 'cheb_random' or colloc_sampling == 'cheb_random_resample':\n",
    "            # Random points equispaced along the circle, then projected to [-1, 1]\n",
    "            i = torch.rand(n_colloc, dtype=torch.float64)\n",
    "            points = torch.cos(torch.pi * i)\n",
    "            # Sort points for better visualization\n",
    "            points, _ = torch.sort(points)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {colloc_sampling}\")\n",
    "        return points\n",
    "    \n",
    "    # Initial collocation points\n",
    "    colloc_points = get_colloc_points()\n",
    "\n",
    "    # Eval points\n",
    "    x_eval = torch.linspace(-1, 1, 200, dtype=torch.float64)\n",
    "\n",
    "    # True soln\n",
    "    lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "    lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "    u_eval = lambda_u(x_eval)\n",
    "    du_eval = lambda_du(x_eval)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'loss': [], 'pde_residual': [], 'bc_residual': [], 'colloc_points': colloc_points, 'u_error_max': [], 'du_error_max': []}\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "        # Eval\n",
    "        u_pred_eval, du_pred_eval = compute_derivative(model, x_eval, eval_mode=True)\n",
    "        \n",
    "        u_error = torch.abs(u_pred_eval - u_eval)\n",
    "        du_error = torch.abs(du_pred_eval - du_eval)\n",
    "        history['u_error_max'].append(torch.max(u_error).item())\n",
    "        history['du_error_max'].append(torch.max(du_error).item())\n",
    "        \n",
    "        # Resample points if using random sampling\n",
    "        if 'resample' in colloc_sampling:\n",
    "            colloc_points = get_colloc_points()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, pde_residual, bc_residual = compute_pde_loss(\n",
    "            model, colloc_points, boundary_weight, u0\n",
    "        )\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record history\n",
    "        history['loss'].append(loss.item())\n",
    "        history['pde_residual'].append(torch.mean(pde_residual**2).item())\n",
    "        history['bc_residual'].append(bc_residual.item()**2)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.2e}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def compute_relative_l2_error(pred, true):\n",
    "    \"\"\"\n",
    "    Compute relative L2 error between predicted and true values.\n",
    "    \n",
    "    Args:\n",
    "        pred: predicted values (torch.Tensor)\n",
    "        true: true values (torch.Tensor)\n",
    "        \n",
    "    Returns:\n",
    "        float: relative L2 error\n",
    "    \"\"\"\n",
    "    return (torch.sqrt(torch.mean((pred - true)**2)) / \n",
    "            torch.sqrt(torch.mean(true**2))).item()\n",
    "\n",
    "def plot_solution(model, history, lambda_u, lambda_du):\n",
    "    \"\"\"Plot the learned solution and compare with true solution\"\"\"\n",
    "\n",
    "    # Plot points\n",
    "    x = torch.linspace(-1, 1, 200, dtype=torch.float64)\n",
    "\n",
    "    # Collocation points\n",
    "    colloc_points = history['colloc_points']\n",
    "    \n",
    "    # True solution\n",
    "    true_u = lambda_u(x)\n",
    "    true_du = lambda_du(x)\n",
    "\n",
    "    # Model prediction\n",
    "    u, du = compute_derivative(model, x, eval_mode=True)\n",
    "    u_colloc, du_colloc = compute_derivative(model, colloc_points, eval_mode=True)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Solution\n",
    "    plt.subplot(131)\n",
    "    plt.plot(x, u, label='Learned')\n",
    "    plt.plot(x, true_u, '--', label='True')\n",
    "    plt.scatter(colloc_points, u_colloc, color='red')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title('Solution')\n",
    "    \n",
    "    # Derivative\n",
    "    plt.subplot(132)\n",
    "    if du is not None:\n",
    "        plt.plot(x, du, label=\"Learned u'\")\n",
    "    plt.plot(x, true_du, '--', label=\"True u'\")\n",
    "    plt.scatter(colloc_points, du_colloc, color='red')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title(\"Derivative\")\n",
    "    \n",
    "    # Error\n",
    "    plt.subplot(133)\n",
    "    error = torch.abs(u - true_u)\n",
    "    if du is not None:\n",
    "        deriv_error = torch.abs(du - true_du)\n",
    "        # plt.semilogy(x, error, label='Solution Error')\n",
    "        # plt.semilogy(x, deriv_error, label='Derivative Error')\n",
    "    if du is not None:\n",
    "        error = torch.abs(du_colloc - u_colloc - colloc_points)\n",
    "        plt.semilogy(colloc_points, error, label='PDE Error')\n",
    "        plt.scatter(colloc_points, error, color='red')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title('Error')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print maximum errors\n",
    "    print(f\"Maximum solution error: {torch.max(error):.2e}\")\n",
    "    if du is not None:\n",
    "        print(f\"Maximum derivative error: {torch.max(deriv_error):.2e}\")\n",
    "\n",
    "    # Print relative l2 errors\n",
    "    print(f\"Relative L2 error for solution: {compute_relative_l2_error(u, true_u):.2e}\")\n",
    "    if du is not None:\n",
    "        print(f\"Relative L2 error for derivative: {compute_relative_l2_error(du, true_du):.2e}\")\n",
    "    # Relative l2 errors at collocation points\n",
    "    print(f\"Relative L2 error at colloc points for solution: {compute_relative_l2_error(u_colloc, lambda_u(colloc_points)):.2e}\")\n",
    "    if du is not None:\n",
    "        print(f\"Relative L2 error at colloc points for derivative: {compute_relative_l2_error(du_colloc, lambda_du(colloc_points)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a275a5",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e78a97",
   "metadata": {},
   "source": [
    "N=11, N_colloc=65, model=interpolant. Change the collocation.\n",
    "- Fixed: uniform, cheb.\n",
    "- Sampling once: uniform, cheb.\n",
    "- Resampling: uniform, cheb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd514d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 65\n",
    "n_epochs = 30000\n",
    "colloc_sampling = 'equispaced'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4359968",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 65\n",
    "n_epochs = 30000\n",
    "colloc_sampling = 'chebyshev'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55442f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 65\n",
    "n_epochs = 30000\n",
    "colloc_sampling = 'random'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 65\n",
    "n_epochs = 30000\n",
    "colloc_sampling = 'cheb_random'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05123c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 65\n",
    "n_epochs = 30000\n",
    "colloc_sampling = 'random_resample'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc503c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 65\n",
    "n_epochs = 30000\n",
    "colloc_sampling = 'cheb_random_resample'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c8e6d",
   "metadata": {},
   "source": [
    "N_colloc=65, model=MLP. Change the collocation.\n",
    "- Resampling: uniform, cheb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa7387",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "hidden_dim = 64\n",
    "n_colloc = 65\n",
    "n_epochs = 30000\n",
    "colloc_sampling = 'random_resample'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = MLP(hidden_dim=hidden_dim, activation=torch.tanh)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43fec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "hidden_dim = 64\n",
    "n_colloc = 65\n",
    "n_epochs = 30000\n",
    "colloc_sampling = 'cheb_random_resample'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = MLP(hidden_dim=hidden_dim, activation=torch.tanh)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f660a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "hidden_dim = 64\n",
    "n_colloc = 65\n",
    "n_epochs = 30000\n",
    "colloc_sampling = 'chebyshev'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = MLP(hidden_dim=hidden_dim, activation=torch.tanh)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed68f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b145f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5d0c9e5",
   "metadata": {},
   "source": [
    "### N=21, N_colloc=21, model=interpolant, sampling=cheb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 21\n",
    "n_colloc = 21\n",
    "n_epochs = 90000\n",
    "colloc_sampling = 'chebyshev'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['loss'], label='Total Loss')\n",
    "plt.semilogy(history['bc_residual'], label='BC Residual')\n",
    "plt.semilogy(history['pde_residual'], label='PDE Residual')\n",
    "plt.semilogy(history['u_error_max'], label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(history['du_error_max'], label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c8875",
   "metadata": {},
   "source": [
    "### N=41, N_colloc=41, model=interpolant, sampling=cheb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 41\n",
    "n_colloc = 61\n",
    "n_epochs = 90000\n",
    "colloc_sampling = 'chebyshev'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-2,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['loss'], label='Total Loss')\n",
    "plt.semilogy(history['bc_residual'], label='BC Residual')\n",
    "plt.semilogy(history['pde_residual'], label='PDE Residual')\n",
    "plt.semilogy(history['u_error_max'], label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(history['du_error_max'], label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896dd2c",
   "metadata": {},
   "source": [
    "### N=11, N_colloc=11, model=interpolant, sampling=random_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 11\n",
    "n_epochs = 90000\n",
    "colloc_sampling = 'random_resample'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['loss'], label='Total Loss')\n",
    "plt.semilogy(history['bc_residual'], label='BC Residual')\n",
    "plt.semilogy(history['pde_residual'], label='PDE Residual')\n",
    "plt.semilogy(history['u_error_max'], label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(history['du_error_max'], label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe09249",
   "metadata": {},
   "source": [
    "### N=11, N_colloc=11, model=interpolant, sampling=cheb_random_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 11\n",
    "n_epochs = 90000\n",
    "colloc_sampling = 'cheb_random_resample'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['loss'], label='Total Loss')\n",
    "plt.semilogy(history['bc_residual'], label='BC Residual')\n",
    "plt.semilogy(history['pde_residual'], label='PDE Residual')\n",
    "plt.semilogy(history['u_error_max'], label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(history['du_error_max'], label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9ea77",
   "metadata": {},
   "source": [
    "### N=11, N_colloc=11, model=interpolant, sampling=cheb_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5677a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 11\n",
    "n_epochs = 90000\n",
    "colloc_sampling = 'cheb_random'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['loss'], label='Total Loss')\n",
    "plt.semilogy(history['bc_residual'], label='BC Residual')\n",
    "plt.semilogy(history['pde_residual'], label='PDE Residual')\n",
    "plt.semilogy(history['u_error_max'], label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(history['du_error_max'], label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38979c2",
   "metadata": {},
   "source": [
    "### N=11, N_colloc=41, model=interpolant, sampling=cheb_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 41\n",
    "n_epochs = 90000\n",
    "colloc_sampling = 'cheb_random'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['loss'], label='Total Loss')\n",
    "plt.semilogy(history['bc_residual'], label='BC Residual')\n",
    "plt.semilogy(history['pde_residual'], label='PDE Residual')\n",
    "plt.semilogy(history['u_error_max'], label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(history['du_error_max'], label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c610280",
   "metadata": {},
   "source": [
    "### N=11, N_colloc=41, model=interpolant, sampling=cheb_random_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 41\n",
    "n_epochs = 90000\n",
    "colloc_sampling = 'cheb_random_resample'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['loss'], label='Total Loss')\n",
    "plt.semilogy(history['bc_residual'], label='BC Residual')\n",
    "plt.semilogy(history['pde_residual'], label='PDE Residual')\n",
    "plt.semilogy(history['u_error_max'], label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(history['du_error_max'], label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9449000",
   "metadata": {},
   "source": [
    "### N=11, N_colloc=41, model=interpolant, sampling=cheb_random, boundary_weight=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a70b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 11\n",
    "n_colloc = 41\n",
    "n_epochs = 90000\n",
    "colloc_sampling = 'cheb_random'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = LagrangeInterpolationModel(n_points)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=0.1,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['loss'], label='Total Loss')\n",
    "plt.semilogy(history['bc_residual'], label='BC Residual')\n",
    "plt.semilogy(history['pde_residual'], label='PDE Residual')\n",
    "plt.semilogy(history['u_error_max'], label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(history['du_error_max'], label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54229a",
   "metadata": {},
   "source": [
    "### N=11, N_colloc=41, model=MLP, sampling=cheb_random, boundary_weight=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860873fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "hidden_dim = 64\n",
    "n_colloc = 41\n",
    "n_epochs = 90000\n",
    "colloc_sampling = 'cheb_random'\n",
    "u0 = 1\n",
    "\n",
    "# u' = u + x, u(0) = u0 has solution u(x) = (u0+1)e^x - x - 1\n",
    "lambda_u = lambda x : (u0+1)*torch.exp(x) - x - 1\n",
    "lambda_du = lambda x : (u0+1)*torch.exp(x) - 1\n",
    "\n",
    "# Create model\n",
    "model = MLP(hidden_dim=hidden_dim, activation=torch.tanh)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(history['loss'], label='Total Loss')\n",
    "plt.semilogy(history['bc_residual'], label='BC Residual')\n",
    "plt.semilogy(history['pde_residual'], label='PDE Residual')\n",
    "plt.semilogy(history['u_error_max'], label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(history['du_error_max'], label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456014e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icon",
   "language": "python",
   "name": "icon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
