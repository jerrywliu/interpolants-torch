{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd31086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53966be0",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InterpolationDataset: for sanity-checking that the networks can interpolate the GT solution function\n",
    "class InterpolationDataset(Dataset):\n",
    "    def __init__(self, num_points, domain=(-1, 1), function=torch.sin, sampling='equispaced', noise_std=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_points: Number of points to sample\n",
    "            domain: Tuple of (min, max) for the domain\n",
    "            function: Function to apply to x values\n",
    "            sampling: String indicating sampling method ('equispaced', 'chebyshev', or 'random')\n",
    "        \"\"\"\n",
    "        self.num_points = num_points\n",
    "        self.domain = domain\n",
    "        self.function = function\n",
    "        self.sampling = sampling\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        # Sample\n",
    "        self.resample()\n",
    "    \n",
    "    def _generate_points(self):\n",
    "        valid_methods = {'equispaced', 'chebyshev', 'random'}\n",
    "        if self.sampling not in valid_methods:\n",
    "            raise ValueError(f\"Sampling method must be one of {valid_methods}\")\n",
    "        if self.sampling == 'equispaced':\n",
    "            x = torch.linspace(self.domain[0], self.domain[1], self.num_points)\n",
    "        elif self.sampling == 'chebyshev':\n",
    "            j = torch.arange(self.num_points)\n",
    "            x = torch.cos((2*j + 1) * torch.pi / (2*self.num_points))\n",
    "            x = self._scale_to_domain(x, self.domain)\n",
    "        else:  # random\n",
    "            x = torch.rand(self.num_points) * (self.domain[1] - self.domain[0]) + self.domain[0]\n",
    "        return x\n",
    "    \n",
    "    def _scale_to_domain(self, x, domain):\n",
    "        \"\"\"Scale points from [-1, 1] to specified domain\"\"\"\n",
    "        return (domain[1] - domain[0]) * (x + 1) / 2 + domain[0]\n",
    "    \n",
    "    def resample(self):\n",
    "        \"\"\"Regenerate x points and corresponding y values\"\"\"\n",
    "        self.x = self._generate_points()\n",
    "        self.y = self.function(self.x)\n",
    "        self.y += torch.randn_like(self.y) * self.noise_std\n",
    "        return self\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a98be",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128f0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=32, activation=torch.tanh):\n",
    "        \"\"\"\n",
    "        2-layer MLP that maps R -> R\n",
    "        \n",
    "        Args:\n",
    "            hidden_dim: Dimension of hidden layer\n",
    "            activation: Activation function to use (default: ReLU)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ensure input is 2D tensor [batch_size, 1]\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(-1)\n",
    "            \n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze(-1)  # Return [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa131380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagrangeInterpolationModel(nn.Module):\n",
    "    def __init__(self, num_points, domain_min=-1, domain_max=1):\n",
    "        \"\"\"\n",
    "        Model parameterized by values at Chebyshev points of the second kind\n",
    "        \n",
    "        Args:\n",
    "            num_points: Number of points (N where N is number of points)\n",
    "            domain_min: Left endpoint of physical domain\n",
    "            domain_max: Right endpoint of physical domain\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.domain_min = domain_min\n",
    "        self.domain_max = domain_max\n",
    "        self.domain_length = domain_max - domain_min\n",
    "        \n",
    "        # Generate Chebyshev points of the second kind in [-1, 1]\n",
    "        i = torch.linspace(0, 1, num_points)\n",
    "        self.nodes_standard = torch.cos(torch.pi * i)  # [-1, 1]\n",
    "        # Map to physical domain\n",
    "        self.nodes = self._from_standard(self.nodes_standard)\n",
    "        \n",
    "        # Learnable values at these points\n",
    "        self.values = nn.Parameter(torch.zeros(num_points))\n",
    "        \n",
    "        # Precompute barycentric weights for Chebyshev points\n",
    "        self.weights = torch.zeros(num_points)\n",
    "        self.weights[::2] = 1\n",
    "        self.weights[1::2] = -1\n",
    "        self.weights[0] = 0.5\n",
    "        self.weights[-1] = 0.5\n",
    "        \n",
    "        # Cache for differentiation matrices\n",
    "        self._diff_matrices = {}\n",
    "        \n",
    "    def _to_standard(self, x):\n",
    "        \"\"\"Map from physical domain to [-1, 1]\"\"\"\n",
    "        return 2 * (x - self.domain_min) / self.domain_length - 1\n",
    "    \n",
    "    def _from_standard(self, x):\n",
    "        \"\"\"Map from [-1, 1] to physical domain\"\"\"\n",
    "        return self.domain_min + (x + 1) * self.domain_length / 2\n",
    "    \n",
    "    def _cheb_interpolate(self, x_eval, values, eps=1e-14):\n",
    "        \"\"\"\n",
    "        Interpolate values using barycentric formula\n",
    "        \"\"\"\n",
    "        # Map evaluation points to standard domain\n",
    "        x_eval_standard = self._to_standard(x_eval)\n",
    "        \n",
    "        # Compute difference matrix (N, N_eval)\n",
    "        d_x = x_eval_standard.unsqueeze(0) - self.nodes_standard.unsqueeze(1)\n",
    "        \n",
    "        # Handle numerical instability for small differences\n",
    "        small_diff = torch.abs(d_x) < eps\n",
    "        small_diff_max = torch.max(small_diff, dim=0).values\n",
    "        \n",
    "        # If small_diff, set the column to 0 and the entry to 1\n",
    "        d_x = torch.where(\n",
    "            small_diff_max[None, :],\n",
    "            torch.zeros_like(d_x),\n",
    "            1.0 / d_x\n",
    "        )\n",
    "        d_x[small_diff] = 1\n",
    "        \n",
    "        # Interpolate\n",
    "        f_eval_num = torch.einsum(\"...n,nm,n->...m\", values, d_x, self.weights)\n",
    "        f_eval_denom = torch.einsum(\"nm,n->m\", d_x, self.weights)\n",
    "        \n",
    "        return f_eval_num / f_eval_denom\n",
    "\n",
    "    def _compute_derivative_matrix(self, nodes):\n",
    "        \"\"\"\n",
    "        Compute the spectral differentiation matrix for Chebyshev points\n",
    "        \"\"\"\n",
    "        n = len(nodes)\n",
    "        D = torch.zeros((n, n), dtype=nodes.dtype, device=nodes.device)\n",
    "        \n",
    "        # Compute weights for endpoints\n",
    "        c = torch.ones(n, dtype=nodes.dtype, device=nodes.device)\n",
    "        c[0] = 2\n",
    "        c[-1] = 2\n",
    "        \n",
    "        # Compute off-diagonal entries\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    D[i,j] = c[i]/c[j] * (-1)**(i+j) / (nodes[i] - nodes[j])\n",
    "        \n",
    "        # Fill diagonal using negative sum trick\n",
    "        D.diagonal().copy_(-torch.sum(D, dim=1))\n",
    "        \n",
    "        # Scale for domain transformation\n",
    "        D = D * (2.0/self.domain_length)\n",
    "        \n",
    "        return D\n",
    "        \n",
    "    def derivative_matrix(self, k=1):\n",
    "        \"\"\"\n",
    "        Get k-th derivative matrix (cached for efficiency)\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            n = len(self.nodes_standard)\n",
    "            return torch.eye(n, dtype=self.nodes_standard.dtype, device=self.nodes_standard.device)\n",
    "            \n",
    "        if k not in self._diff_matrices:\n",
    "            # Compute first derivative matrix if not cached\n",
    "            if 1 not in self._diff_matrices:\n",
    "                self._diff_matrices[1] = self._compute_derivative_matrix(self.nodes_standard)\n",
    "            \n",
    "            # Compute k-th derivative matrix by composition\n",
    "            Dk = self._diff_matrices[1]\n",
    "            for i in range(k-1):\n",
    "                Dk = Dk @ self._diff_matrices[1]\n",
    "            self._diff_matrices[k] = Dk\n",
    "            \n",
    "        return self._diff_matrices[k]\n",
    "        \n",
    "    def derivative(self, x_eval, k=1):\n",
    "        \"\"\"\n",
    "        Compute k-th derivative of interpolant at x_eval points\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            return self(x_eval)\n",
    "            \n",
    "        # Get k-th derivative matrix\n",
    "        Dk = self.derivative_matrix(k)\n",
    "        \n",
    "        # Compute derivative values at nodes (differentiable w.r.t self.values)\n",
    "        dk_nodes = Dk @ self.values\n",
    "        \n",
    "        # Interpolate to evaluation points\n",
    "        return self._cheb_interpolate(x_eval, dk_nodes)\n",
    "    \n",
    "    def forward(self, x_eval):\n",
    "        return self._cheb_interpolate(x_eval, self.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4007e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation uses FFT to compute derivatives\n",
    "class FourierInterpolationModel(nn.Module):\n",
    "    def __init__(self, num_points, domain_min=-1, domain_max=1):\n",
    "        \"\"\"\n",
    "        Model parameterized by values at equispaced points with periodic boundary conditions\n",
    "        \n",
    "        Args:\n",
    "            num_points: Number of points (N where N is number of points)\n",
    "            domain_min: Left endpoint of physical domain\n",
    "            domain_max: Right endpoint of physical domain\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.domain_min = domain_min\n",
    "        self.domain_max = domain_max\n",
    "        self.domain_length = domain_max - domain_min\n",
    "        \n",
    "        # Generate equispaced points in [0, 2π) \n",
    "        self.nodes_standard = torch.linspace(0, 2*np.pi, num_points+1)[:-1]\n",
    "        # Map to physical domain\n",
    "        self.nodes = self._from_standard(self.nodes_standard)\n",
    "        \n",
    "        # Learnable values at these points\n",
    "        self.values = nn.Parameter(torch.zeros(num_points))\n",
    "        \n",
    "        # Precompute wavenumbers\n",
    "        self.k = torch.fft.fftfreq(num_points) * num_points\n",
    "    \n",
    "    def _to_standard(self, x):\n",
    "        \"\"\"Map from physical domain to [0, 2π)\"\"\"\n",
    "        return 2*np.pi * (x - self.domain_min) / self.domain_length\n",
    "    \n",
    "    def _from_standard(self, x):\n",
    "        \"\"\"Map from [0, 2π) to physical domain\"\"\"\n",
    "        return self.domain_min + self.domain_length * x / (2*np.pi)\n",
    "    \n",
    "    def _fourier_interpolate(self, x_eval, values):\n",
    "        \"\"\"\n",
    "        Interpolate values using FFT\n",
    "        \"\"\"\n",
    "        N = len(values)\n",
    "        \n",
    "        # Get Fourier coefficients\n",
    "        coeffs = torch.fft.fft(values)\n",
    "        \n",
    "        # Map x_eval to standard domain [0,2π]\n",
    "        x_eval_standard = self._to_standard(x_eval)\n",
    "        \n",
    "        # Evaluate Fourier series\n",
    "        x_matrix = x_eval_standard.unsqueeze(1) * self.k.unsqueeze(0)\n",
    "        fourier_matrix = torch.exp(1j * x_matrix)\n",
    "        result = torch.real(fourier_matrix @ coeffs) / N\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def derivative(self, x_eval, k=1):\n",
    "        \"\"\"\n",
    "        Compute k-th derivative using FFT\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            return self(x_eval)\n",
    "            \n",
    "        N = len(self.values)\n",
    "        \n",
    "        # Get Fourier coefficients\n",
    "        coeffs = torch.fft.fft(self.values)\n",
    "        \n",
    "        # Map x_eval to standard domain [0,2π]\n",
    "        x_eval_standard = self._to_standard(x_eval)\n",
    "        \n",
    "        # Multiply by (ik)^k in Fourier space and scale for domain transformation\n",
    "        # Each derivative brings down a factor of 2π/L from the chain rule\n",
    "        scale = (2*np.pi/self.domain_length) ** k\n",
    "        dk_coeffs = coeffs * (1j * self.k) ** k * scale\n",
    "        \n",
    "        # Evaluate derivative\n",
    "        x_matrix = x_eval_standard.unsqueeze(1) * self.k.unsqueeze(0)\n",
    "        fourier_matrix = torch.exp(1j * x_matrix)\n",
    "        result = torch.real(fourier_matrix @ dk_coeffs) / N\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def forward(self, x_eval):\n",
    "        return self._fourier_interpolate(x_eval, self.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd32b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version produces the differentiation matrix explicitly\n",
    "class FourierInterpolationModel(nn.Module):\n",
    "    def __init__(self, num_points, domain_min=-1, domain_max=1):\n",
    "        \"\"\"\n",
    "        Model parameterized by values at equispaced points with periodic boundary conditions\n",
    "        \n",
    "        Args:\n",
    "            num_points: Number of points (N where N is number of points)\n",
    "            domain_min: Left endpoint of physical domain\n",
    "            domain_max: Right endpoint of physical domain\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.domain_min = domain_min\n",
    "        self.domain_max = domain_max\n",
    "        self.domain_length = domain_max - domain_min\n",
    "        \n",
    "        # Generate equispaced points in [0, 2π) \n",
    "        self.nodes_standard = torch.linspace(0, 2*np.pi, num_points+1)[:-1]\n",
    "        # Map to physical domain\n",
    "        self.nodes = self._from_standard(self.nodes_standard)\n",
    "        \n",
    "        # Learnable values at these points\n",
    "        self.values = nn.Parameter(torch.zeros(num_points))\n",
    "        \n",
    "        # Precompute wavenumbers\n",
    "        self.k = torch.fft.fftfreq(num_points) * num_points\n",
    "        \n",
    "        # Cache for differentiation matrices\n",
    "        self._diff_matrices = {}\n",
    "        \n",
    "    def _to_standard(self, x):\n",
    "        \"\"\"Map from physical domain to [0, 2π)\"\"\"\n",
    "        return 2*np.pi * (x - self.domain_min) / self.domain_length\n",
    "    \n",
    "    def _from_standard(self, x):\n",
    "        \"\"\"Map from [0, 2π) to physical domain\"\"\"\n",
    "        return self.domain_min + self.domain_length * x / (2*np.pi)\n",
    "    \n",
    "    def _compute_derivative_matrix(self, nodes):\n",
    "        \"\"\"\n",
    "        Compute the Fourier differentiation matrix using cotangent formula\n",
    "        \"\"\"\n",
    "        N = len(nodes)\n",
    "        D = torch.zeros((N, N), dtype=nodes.dtype, device=nodes.device)\n",
    "        \n",
    "        # Create index matrices\n",
    "        i, j = torch.meshgrid(torch.arange(N, dtype=nodes.dtype, device=nodes.device), \n",
    "                            torch.arange(N, dtype=nodes.dtype, device=nodes.device), \n",
    "                            indexing='ij')\n",
    "        \n",
    "        # Compute off-diagonal elements using cotangent formula\n",
    "        mask = i != j\n",
    "        diff = (i[mask] - j[mask]) * (-1)**(i[mask] - j[mask])\n",
    "        D[mask] = 0.5 * torch.tan(torch.pi * diff / N).reciprocal()\n",
    "        \n",
    "        # Diagonal elements are 0 for periodic functions\n",
    "        D.diagonal().zero_()\n",
    "        \n",
    "        # Scale for domain transformation\n",
    "        D = D * (2*np.pi/self.domain_length)\n",
    "        \n",
    "        return D\n",
    "    \n",
    "    def derivative_matrix(self, k=1):\n",
    "        \"\"\"\n",
    "        Get k-th derivative matrix (cached for efficiency)\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            n = len(self.nodes)\n",
    "            return torch.eye(n, dtype=self.nodes.dtype, device=self.nodes.device)\n",
    "            \n",
    "        if k not in self._diff_matrices:\n",
    "            # Compute first derivative matrix if not cached\n",
    "            if 1 not in self._diff_matrices:\n",
    "                self._diff_matrices[1] = self._compute_derivative_matrix(self.nodes)\n",
    "            \n",
    "            # Compute k-th derivative matrix by composition\n",
    "            Dk = self._diff_matrices[1]\n",
    "            for i in range(k-1):\n",
    "                Dk = Dk @ self._diff_matrices[1]\n",
    "            self._diff_matrices[k] = Dk\n",
    "            \n",
    "        return self._diff_matrices[k]\n",
    "    \n",
    "    def _fourier_interpolate(self, x_eval, values):\n",
    "        \"\"\"\n",
    "        Interpolate values using FFT\n",
    "        \"\"\"\n",
    "        N = len(values)\n",
    "        \n",
    "        # Get Fourier coefficients\n",
    "        coeffs = torch.fft.fft(values)\n",
    "        \n",
    "        # Map x_eval to standard domain [0,2π]\n",
    "        x_eval_standard = self._to_standard(x_eval)\n",
    "        \n",
    "        # Evaluate Fourier series\n",
    "        x_matrix = x_eval_standard.unsqueeze(1) * self.k.unsqueeze(0)\n",
    "        fourier_matrix = torch.exp(1j * x_matrix)\n",
    "        result = torch.real(fourier_matrix @ coeffs) / N\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def derivative(self, x_eval, k=1):\n",
    "        \"\"\"\n",
    "        Compute k-th derivative at x_eval points using differentiation matrix\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            return self(x_eval)\n",
    "            \n",
    "        # Get k-th derivative matrix\n",
    "        Dk = self.derivative_matrix(k)\n",
    "        \n",
    "        # Compute derivative values at nodes\n",
    "        dk_nodes = Dk @ self.values\n",
    "        \n",
    "        # Interpolate to evaluation points\n",
    "        return self._fourier_interpolate(x_eval, dk_nodes)\n",
    "    \n",
    "    def forward(self, x_eval):\n",
    "        return self._fourier_interpolate(x_eval, self.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a122b",
   "metadata": {},
   "source": [
    "## Sanity check: create model and set to interpolate exp(sin(x)). Are the derivatives accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0ca56",
   "metadata": {},
   "source": [
    "### Cheb (non-periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 41\n",
    "domain_min = 3\n",
    "domain_max = 5\n",
    "# exp(sin(pi*x))\n",
    "true_funcs = [\n",
    "    lambda x: torch.exp(torch.sin(torch.pi * x)),\n",
    "    lambda x: torch.exp(torch.sin(torch.pi * x)) * torch.pi * torch.cos(torch.pi * x),\n",
    "    lambda x: torch.exp(torch.sin(torch.pi * x)) * (torch.pi**2) * (torch.cos(torch.pi * x)**2 - torch.sin(torch.pi * x)),\n",
    "    lambda x: torch.exp(torch.sin(torch.pi * x)) * (torch.pi**3) * (torch.cos(torch.pi * x)**3 - 3*torch.cos(torch.pi * x)*torch.sin(torch.pi * x) - torch.cos(torch.pi * x))\n",
    "]\n",
    "\n",
    "model = LagrangeInterpolationModel(n_points, domain_min=domain_min, domain_max=domain_max)\n",
    "model.values.data = true_funcs[0](model.nodes)\n",
    "\n",
    "# Test points - include both endpoints for visualization\n",
    "x_eval = torch.linspace(domain_min, domain_max, 200)\n",
    "\n",
    "# Compute derivatives\n",
    "derivs = [model.derivative(x_eval, k) for k in range(4)]\n",
    "true_derivs = [f(x_eval) for f in true_funcs]\n",
    "\n",
    "# Plot errors\n",
    "plt.figure(figsize=(15, 5))\n",
    "for k in range(4):\n",
    "    error = torch.abs(derivs[k] - true_derivs[k])\n",
    "    plt.semilogy(x_eval, error.detach().cpu().numpy(), label=f'{k}-th derivative')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Derivative Errors (Chebyshev)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.show()\n",
    "\n",
    "# Print max errors\n",
    "print(\"\\nMaximum errors:\")\n",
    "for k in range(4):\n",
    "    error = torch.max(torch.abs(derivs[k] - true_derivs[k]))\n",
    "    print(f\"{k}-th derivative: {error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61686ffe",
   "metadata": {},
   "source": [
    "### Fourier (periodic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 40\n",
    "domain_min = 3\n",
    "domain_max = 5\n",
    "test_function = lambda x : torch.exp(torch.sin(torch.pi * x))\n",
    "\n",
    "model = FourierInterpolationModel(n_points, domain_min=domain_min, domain_max=domain_max)\n",
    "model.values.data = test_function(model.nodes)\n",
    "\n",
    "# Create fine grid for evaluation\n",
    "x_eval = torch.linspace(domain_min, domain_max, 200)\n",
    "y_eval = model(x_eval)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot interpolation\n",
    "plt.plot(x_eval, y_eval.detach(), 'b-', label='Interpolation')\n",
    "\n",
    "# Plot true function\n",
    "y_true = test_function(x_eval)\n",
    "plt.plot(x_eval, y_true, 'r--', label='True sin(πx)')\n",
    "\n",
    "# Plot nodes\n",
    "plt.plot(model.nodes, model.values.detach(), 'ko', label='Nodes')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Trigonometric Interpolation Test')\n",
    "\n",
    "# Plot error\n",
    "plt.figure(figsize=(12, 4))\n",
    "error = torch.abs(y_eval - y_true)\n",
    "plt.semilogy(x_eval, error.detach(), label='Error')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Interpolation Error')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print maximum error\n",
    "print(f\"Maximum error: {error.max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d005fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 40\n",
    "domain_min = 3\n",
    "domain_max = 5\n",
    "# exp(sin(pi*x))\n",
    "true_funcs = [\n",
    "    lambda x: torch.exp(torch.sin(torch.pi * x)),\n",
    "    lambda x: torch.exp(torch.sin(torch.pi * x)) * torch.pi * torch.cos(torch.pi * x),\n",
    "    lambda x: torch.exp(torch.sin(torch.pi * x)) * (torch.pi**2) * (torch.cos(torch.pi * x)**2 - torch.sin(torch.pi * x)),\n",
    "    lambda x: torch.exp(torch.sin(torch.pi * x)) * (torch.pi**3) * (torch.cos(torch.pi * x)**3 - 3*torch.cos(torch.pi * x)*torch.sin(torch.pi * x) - torch.cos(torch.pi * x))\n",
    "]\n",
    "\n",
    "model = FourierInterpolationModel(n_points, domain_min=domain_min, domain_max=domain_max)\n",
    "model.values.data = true_funcs[0](model.nodes)\n",
    "\n",
    "# Test points - include both endpoints for visualization\n",
    "x_eval = torch.linspace(domain_min, domain_max, 200)\n",
    "\n",
    "# Compute derivatives\n",
    "derivs = [model.derivative(x_eval, k) for k in range(4)]\n",
    "true_derivs = [f(x_eval) for f in true_funcs]\n",
    "\n",
    "# Plot errors\n",
    "plt.figure(figsize=(15, 5))\n",
    "for k in range(4):\n",
    "    error = torch.abs(derivs[k] - true_derivs[k])\n",
    "    plt.semilogy(x_eval, error.detach().cpu().numpy(), label=f'{k}-th derivative')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Derivative Errors (Fourier)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.show()\n",
    "\n",
    "# Print max errors\n",
    "print(\"\\nMaximum errors:\")\n",
    "for k in range(4):\n",
    "    error = torch.max(torch.abs(derivs[k] - true_derivs[k]))\n",
    "    print(f\"{k}-th derivative: {error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d4c02",
   "metadata": {},
   "source": [
    "# PINN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivative(model, x, eval_mode=False):\n",
    "    if isinstance(model, LagrangeInterpolationModel) or isinstance(model, FourierInterpolationModel):\n",
    "        u = model(x)\n",
    "        du = model.derivative(x, k=1)\n",
    "    else:\n",
    "        # For MLP, compute gradient manually\n",
    "        x_clone = x.clone().requires_grad_(True)\n",
    "        u = model(x_clone)\n",
    "        # During eval, we don't need create_graph\n",
    "        du = torch.autograd.grad(u.sum(), x_clone, \n",
    "                               create_graph=not eval_mode)[0]\n",
    "        if eval_mode:\n",
    "            u = u.detach()\n",
    "            du = du.detach()\n",
    "    return u, du\n",
    "\n",
    "def compute_pde_loss(model, colloc_points, boundary_weight=1.0, u0=1.0):\n",
    "    \"\"\"\n",
    "    Compute loss for ODE u' = 2π*cos(πx)*u with u(0) = 1\n",
    "    \"\"\"\n",
    "    u, du = compute_derivative(model, colloc_points)\n",
    "        \n",
    "    # PDE residual: u' = 2π*cos(πx)*u\n",
    "    pde_residual = du - 2*np.pi*torch.cos(np.pi*colloc_points)*u\n",
    "        \n",
    "    pde_loss = torch.mean(pde_residual**2)\n",
    "    \n",
    "    # Boundary condition: u(0) = 1\n",
    "    bc_point = torch.tensor([0.0], dtype=torch.float64)\n",
    "    bc_residual = model(bc_point) - u0\n",
    "    bc_loss = boundary_weight * bc_residual**2\n",
    "    \n",
    "    return pde_loss + bc_loss, pde_residual, bc_residual\n",
    "\n",
    "# Simple iterative approach\n",
    "def running_min(lst):\n",
    "    result = []\n",
    "    current_min = float('inf')\n",
    "    for x in lst:\n",
    "        current_min = min(current_min, x)\n",
    "        result.append(current_min)\n",
    "    return result\n",
    "\n",
    "def train_pinn(model, n_colloc=100, n_epochs=1000, lr=1e-3, boundary_weight=1.0, \n",
    "                colloc_sampling='equispaced', u0=0):\n",
    "    \"\"\"\n",
    "    Train model to solve the ODE\n",
    "    \n",
    "    Args:\n",
    "        model: neural network model\n",
    "        n_colloc: number of collocation points\n",
    "        n_epochs: number of training epochs\n",
    "        lr: learning rate\n",
    "        boundary_weight: weight for boundary condition term\n",
    "        colloc_sampling: sampling strategy for collocation points\n",
    "            - 'equispaced': evenly spaced points\n",
    "            - 'chebyshev': Chebyshev points of second kind\n",
    "            - 'random': uniformly random points, not resampled\n",
    "            - 'random_resample': uniformly random points, resampled each epoch\n",
    "    \"\"\"\n",
    "    def get_colloc_points():\n",
    "        if colloc_sampling == 'equispaced':\n",
    "            points = torch.linspace(-1, 1, n_colloc, dtype=torch.float64)\n",
    "        elif colloc_sampling == 'chebyshev':\n",
    "            i = torch.linspace(0, 1, n_colloc, dtype=torch.float64)\n",
    "            points = torch.cos(torch.pi * i)\n",
    "        elif colloc_sampling == 'random' or colloc_sampling == 'random_resample':\n",
    "            # Random points in [-1, 1]\n",
    "            points = 2 * torch.rand(n_colloc, dtype=torch.float64) - 1\n",
    "            # Sort points for better visualization\n",
    "            points, _ = torch.sort(points)\n",
    "        elif colloc_sampling == 'cheb_random' or colloc_sampling == 'cheb_random_resample':\n",
    "            # Random points equispaced along the circle, then projected to [-1, 1]\n",
    "            i = torch.rand(n_colloc, dtype=torch.float64)\n",
    "            points = torch.cos(torch.pi * i)\n",
    "            # Sort points for better visualization\n",
    "            points, _ = torch.sort(points)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {colloc_sampling}\")\n",
    "        return points\n",
    "    \n",
    "    # Initial collocation points\n",
    "    colloc_points = get_colloc_points()\n",
    "\n",
    "    # Eval points\n",
    "    x_eval = torch.linspace(-1, 1, 200, dtype=torch.float64)\n",
    "\n",
    "    # True soln\n",
    "    lambda_u = lambda x: torch.exp(2*torch.sin(np.pi*x))\n",
    "    lambda_du = lambda x: 2*np.pi*torch.cos(np.pi*x)*torch.exp(2*torch.sin(np.pi*x))\n",
    "\n",
    "    u_eval = lambda_u(x_eval)\n",
    "    du_eval = lambda_du(x_eval)\n",
    "    \n",
    "    # Optimizer\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # Training history\n",
    "    history = {'loss': [], 'pde_residual': [], 'bc_residual': [], 'colloc_points': colloc_points, 'u_error_max': [], 'du_error_max': []}\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "\n",
    "        # Eval\n",
    "        u_pred_eval, du_pred_eval = compute_derivative(model, x_eval, eval_mode=True)\n",
    "        \n",
    "        u_error = torch.abs(u_pred_eval - u_eval)\n",
    "        du_error = torch.abs(du_pred_eval - du_eval)\n",
    "        history['u_error_max'].append(torch.max(u_error).item())\n",
    "        history['du_error_max'].append(torch.max(du_error).item())\n",
    "        \n",
    "        # Resample points if using random sampling\n",
    "        if 'resample' in colloc_sampling:\n",
    "            colloc_points = get_colloc_points()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, pde_residual, bc_residual = compute_pde_loss(\n",
    "            model, colloc_points, boundary_weight, u0\n",
    "        )\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record history\n",
    "        history['loss'].append(loss.item())\n",
    "        history['pde_residual'].append(torch.mean(pde_residual**2).item())\n",
    "        history['bc_residual'].append(bc_residual.item()**2)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.2e}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def compute_relative_l2_error(pred, true):\n",
    "    \"\"\"\n",
    "    Compute relative L2 error between predicted and true values.\n",
    "    \n",
    "    Args:\n",
    "        pred: predicted values (torch.Tensor)\n",
    "        true: true values (torch.Tensor)\n",
    "        \n",
    "    Returns:\n",
    "        float: relative L2 error\n",
    "    \"\"\"\n",
    "    return (torch.sqrt(torch.mean((pred - true)**2)) / \n",
    "            torch.sqrt(torch.mean(true**2))).item()\n",
    "\n",
    "def plot_solution(model, history, lambda_u, lambda_du):\n",
    "    \"\"\"Plot the learned solution and compare with true solution\"\"\"\n",
    "\n",
    "    # Plot points\n",
    "    x = torch.linspace(-1, 1, 200, dtype=torch.float64)\n",
    "\n",
    "    # Collocation points\n",
    "    colloc_points = history['colloc_points']\n",
    "    \n",
    "    # True solution\n",
    "    true_u = lambda_u(x)\n",
    "    true_du = lambda_du(x)\n",
    "\n",
    "    # Model prediction\n",
    "    u, du = compute_derivative(model, x, eval_mode=True)\n",
    "    u_colloc, du_colloc = compute_derivative(model, colloc_points, eval_mode=True)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Solution\n",
    "    plt.subplot(131)\n",
    "    plt.plot(x, u.detach(), label='Learned')\n",
    "    plt.plot(x, true_u.detach(), '--', label='True')\n",
    "    plt.scatter(colloc_points, u_colloc.detach(), color='red')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title('Solution')\n",
    "    \n",
    "    # Derivative\n",
    "    plt.subplot(132)\n",
    "    if du is not None:\n",
    "        plt.plot(x, du.detach(), label=\"Learned u'\")\n",
    "    plt.plot(x, true_du.detach(), '--', label=\"True u'\")\n",
    "    plt.scatter(colloc_points, du_colloc.detach(), color='red')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title(\"Derivative\")\n",
    "    \n",
    "    # Error\n",
    "    plt.subplot(133)\n",
    "    error = torch.abs(u - true_u).detach()\n",
    "    if du is not None:\n",
    "        deriv_error = torch.abs(du - true_du).detach()\n",
    "    if du is not None:\n",
    "        error_colloc = torch.abs(du_colloc - 2*np.pi*torch.cos(np.pi*colloc_points)*u_colloc).detach()\n",
    "        plt.semilogy(colloc_points, error_colloc, label='PDE Error')\n",
    "        plt.scatter(colloc_points, error_colloc, color='red')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title('Error')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print maximum errors\n",
    "    print(f\"Maximum solution error: {torch.max(error):.2e}\")\n",
    "    if du is not None:\n",
    "        print(f\"Maximum derivative error: {torch.max(deriv_error):.2e}\")\n",
    "\n",
    "    # Print relative l2 errors\n",
    "    print(f\"Relative L2 error for solution: {compute_relative_l2_error(u.detach(), true_u.detach()):.2e}\")\n",
    "    if du is not None:\n",
    "        print(f\"Relative L2 error for derivative: {compute_relative_l2_error(du.detach(), true_du.detach()):.2e}\")\n",
    "    # Relative l2 errors at collocation points\n",
    "    print(f\"Relative L2 error at colloc points for solution: {compute_relative_l2_error(u_colloc.detach(), lambda_u(colloc_points).detach()):.2e}\")\n",
    "    if du is not None:\n",
    "        print(f\"Relative L2 error at colloc points for derivative: {compute_relative_l2_error(du_colloc.detach(), lambda_du(colloc_points).detach()):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298af4e0",
   "metadata": {},
   "source": [
    "Equation:\n",
    "$$u' = 2\\pi \\cos(2\\pi x) u$$\n",
    "Solution:\n",
    "$$u = C \\exp(\\sin(2\\pi x))$$\n",
    "We set $u(0) = 1$ to get $C = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Experiment params\n",
    "n_points = 40\n",
    "domain_min = -1\n",
    "domain_max = 1\n",
    "n_colloc = 65\n",
    "n_epochs = 100000\n",
    "colloc_sampling = 'equispaced'\n",
    "u0 = 1\n",
    "\n",
    "# u' = 2*pi*cos(2*pi*x)*u, u(0) = 1 has solution u(x) = exp(sin(2*pi*x)), which is periodic on [-1, 1]\n",
    "lambda_u = lambda x: torch.exp(2*torch.sin(np.pi*x))\n",
    "lambda_du = lambda x: 2*np.pi*torch.cos(np.pi*x)*torch.exp(2*torch.sin(np.pi*x))\n",
    "\n",
    "# Create model\n",
    "model = FourierInterpolationModel(n_points, domain_min=domain_min, domain_max=domain_max)\n",
    "\n",
    "# Train\n",
    "history = train_pinn(\n",
    "    model, \n",
    "    n_colloc=n_colloc, \n",
    "    n_epochs=n_epochs,\n",
    "    lr=1e-3,\n",
    "    boundary_weight=1.0,\n",
    "    colloc_sampling=colloc_sampling,\n",
    "    u0=1,\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(running_min(history['loss']), label='Total Loss')\n",
    "plt.semilogy(running_min(history['bc_residual']), label='BC Residual')\n",
    "plt.semilogy(running_min(history['pde_residual']), label='PDE Residual')\n",
    "plt.semilogy(running_min(history['u_error_max']), label='$||u-\\hat{u}||_\\infty$')\n",
    "plt.semilogy(running_min(history['du_error_max']), label=\"$||u'-\\hat{u'}||_\\infty$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot solution comparison\n",
    "plot_solution(model, history, lambda_u, lambda_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251d8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icon",
   "language": "python",
   "name": "icon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
